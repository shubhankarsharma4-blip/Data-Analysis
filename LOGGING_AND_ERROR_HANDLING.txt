â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   PRODUCTION-GRADE LOGGING & ERROR HANDLING GUIDE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your ETL pipeline now has comprehensive logging and error handling that makes
it production-ready. This guide explains what was added and how to use it.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ WHAT WAS ADDED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. ENHANCED LOGGER CONFIGURATION (src/logger_config.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Dual output logging (console + timestamped files)
   âœ… Context managers for operation tracking (log_context)
   âœ… Summary logging helper (log_summary)
   âœ… Production-grade formatting with timestamps
   âœ… Detailed error tracebacks (exc_info=True)

   New Features:
   â€¢ log_context() â€” Context manager for entry/exit logging
     with automatic error tracking and cleanup
   
   â€¢ log_summary() â€” Pretty-print summaries of operations
     with aligned columns for readability
   
   Example Output:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 2025-11-28 11:46:56 | src.extract | INFO | â†’[load_users]â”‚
   â”‚ Starting... source=users.csv                             â”‚
   â”‚ 2025-11-28 11:46:56 | src.extract | INFO | âœ“ Loaded     â”‚
   â”‚ 10000 users                                              â”‚
   â”‚ 2025-11-28 11:46:56 | src.extract | INFO | âœ“[load_users]â”‚
   â”‚ Completed successfully                                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


2. ENHANCED EXTRACT PHASE (src/extract.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Safe CSV loading with error handling
   âœ… Entry/exit logging for each table load
   âœ… File existence checks before loading
   âœ… Detailed error messages for corrupted CSVs
   âœ… Summary statistics (NULL counts, dimensions)
   âœ… CSV parsing error detection

   Logs:
   â€¢ When file not found â†’ WARNING
   â€¢ When bad dates/values â†’ WARNING (with count)
   â€¢ Successful loads â†’ INFO (row count)
   â€¢ Parse errors â†’ ERROR (with exception details)

   Example:
   INFO  | â†’ [load_users] Starting... source=users.csv
   INFO  | âœ“ Loaded 10000 users
   INFO  | âœ“ [load_users] Completed successfully


3. ENHANCED TRANSFORM STAGING (src/transform_staging.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Entry/exit logging for each table transformation
   âœ… Before/after row count tracking
   âœ… Duplicate drop warnings with count
   âœ… Data quality warnings:
      - NULL primary keys
      - Bad dates (invalid formats â†’ NaT)
      - Bad numeric values (non-numeric â†’ NaN)
   âœ… Detailed staging operation logs

   Logs:
   â€¢ NULL primary keys â†’ WARNING
   â€¢ Bad data conversions â†’ WARNING (with count)
   â€¢ Dropped duplicates â†’ INFO
   â€¢ Stage complete â†’ INFO with row count

   Example:
   INFO  | â†’ [stage_users] Starting...
   WARNING | Found 10 NULL user_ids (will cause issues)
   WARNING | 5 invalid signup dates converted to NaT
   INFO  | Users: 100000 â†’ 99985 rows (dropped 15 duplicates)
   INFO  | âœ“ [stage_users] Completed successfully


4. ENHANCED TRANSFORM WAREHOUSE (src/transform_warehouse.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Entry/exit logging for each table build
   âœ… Row Ã— column dimension tracking
   âœ… Derived column logging (new columns added)
   âœ… Missing value calculation with warning
   âœ… Summary of total rows across all tables

   Logs:
   â€¢ Missing calculated values â†’ WARNING
   â€¢ Column creation â†’ DEBUG
   â€¢ Table dimensions â†’ INFO
   â€¢ Total rows summary â†’ INFO

   Example:
   INFO  | â†’ [build_fact_order_items] Starting...
   WARNING | 100 missing item_total values - calculating
   INFO  | fact_order_items: 43525 rows Ã— 7 columns
   INFO  | âœ“ [build_fact_order_items] Completed successfully
   INFO  | Total rows across all tables: 170,525


5. ENHANCED LOAD PHASE (src/load.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… File save verification
   âœ… File size tracking (KB)
   âœ… Empty DataFrame detection
   âœ… Column listing in debug logs
   âœ… Failed save tracking with rollup

   Logs:
   â€¢ Empty tables â†’ WARNING (skip)
   â€¢ Successful saves â†’ INFO (with file size)
   â€¢ Column info â†’ DEBUG
   â€¢ Save failures â†’ ERROR with exception

   Example:
   INFO  | âœ“ dim_users: 10000 rows â†’ dim_users.csv (813.1 KB)
   DEBUG | Columns: user_id, user_name, signup_date, ...
   ERROR | âœ— dim_products: Save failed - Permission denied


6. ENHANCED PIPELINE ORCHESTRATOR (src/pipeline.py)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Stage-by-stage logging with â†’ markers
   âœ… Completion verification per stage
   âœ… Specific error type detection (KeyError, etc.)
   âœ… Full exception tracebacks
   âœ… Graceful failure with helpful messages

   Logs:
   â€¢ Stage start â†’ â†’ STAGE N: [name]
   â€¢ Stage complete â†’ âœ“ Stage complete
   â€¢ Errors â†’ Type-specific messages with traceback
   â€¢ Halt message â†’ "Check logs above for details"

   Example:
   INFO  | â†’ STAGE 1: EXTRACT
   INFO  | âœ“ Extract complete: 6 tables loaded
   INFO  | â†’ STAGE 2: TRANSFORM STAGING
   ERROR | âŒ Pipeline failed: KeyError: 'missing_column'
   ERROR | Pipeline execution halted. Check logs above


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” HOW LOGGING WORKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Log Levels (in order of severity):
  â€¢ DEBUG: Detailed info for developers (file only)
  â€¢ INFO: General informational messages (console + file)
  â€¢ WARNING: Warning messages, data quality issues (console + file)
  â€¢ ERROR: Error messages, exceptions, failures (console + file)

Output Targets:
  â€¢ Console: INFO and above (user-facing)
  â€¢ Log File: DEBUG and above (complete audit trail)
  â€¢ Format: timestamp | module_name | level | message

Log File Location:
  â€¢ Directory: logs/
  â€¢ Filename: etl_YYYYMMDD_HHMMSS.log (new file each run)
  â€¢ Size: Typically 10-50 KB per run
  â€¢ Retention: Keep manually or set up log rotation

Timestamp Format:
  â€¢ Console: 2025-11-28 11:46:56
  â€¢ File: Same as console (ISO format)
  â€¢ Per-second granularity for accurate timing


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ ERROR HANDLING PATTERNS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Pattern 1: Safe Loading with Error Details
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    df = load_csv_safe(filepath, table_name)
except pd.errors.ParserError as e:
    logger.error(f"CSV parsing error in {table_name}: {e}")
    raise
except Exception as e:
    logger.error(f"Unexpected error: {type(e).__name__}: {e}")
    raise

Result:
  â€¢ Specific error type logged
  â€¢ Exception details captured
  â€¢ Full traceback in log file
  â€¢ Pipeline halts with clear error message


Pattern 2: Context Manager for Operations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with log_context(logger, "stage_users", source="users.csv"):
    # Your transformation code
    df = transform_data(df)

Result:
  â€¢ Auto-logs "Starting..."
  â€¢ Catches and logs any exception
  â€¢ Logs "Completed successfully"
  â€¢ Full traceback on error


Pattern 3: Data Quality Warnings
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
null_count = df['user_id'].isna().sum()
if null_count > 0:
    logger.warning(f"Found {null_count} NULL user_ids")

Result:
  â€¢ Visible in console (YELLOW/WARNING level)
  â€¢ Flagged in log file
  â€¢ Pipeline continues (non-blocking)
  â€¢ Helps identify data issues


Pattern 4: Detailed Operation Logging
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
before = len(df)
df = df.drop_duplicates(subset='user_id')
dropped = before - len(df)
logger.info(f"Users: {before} â†’ {len(df)} (dropped {dropped} duplicates)")

Result:
  â€¢ Clear before/after metrics
  â€¢ Easy to spot data loss
  â€¢ Helps with debugging


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š READING LOGS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Quick Review (Last 50 Lines):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PowerShell:
  Get-Content logs\* -Tail 50

Shows:
  â€¢ Pipeline completion status
  â€¢ Final validation results
  â€¢ Log file location


Full Pipeline Trace:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Open the log file directly:
  C:\Python\ecommerce_etl_project\logs\etl_20251128_114656.log

Contains:
  â€¢ Every operation with timestamps
  â€¢ Row counts before/after each stage
  â€¢ All warnings and errors
  â€¢ Full exception tracebacks
  â€¢ Performance metrics


Debug Specific Issues:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Search for:
  â€¢ ERROR: All errors in the run
  â€¢ WARNING: All data quality issues
  â€¢ [stage_name]: All logs for specific stage
  â€¢ NULL: NULL value warnings
  â€¢ duplicate: Duplicate drop counts


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ PRODUCTION DEPLOYMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When Scheduling (e.g., Windows Task Scheduler):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Command: python run_all.py
Working Directory: C:\Python\ecommerce_etl_project

Monitoring:
  â€¢ Exit Code 0 = Success âœ…
  â€¢ Exit Code 1 = Failure âŒ
  â€¢ Check logs/ directory for latest run
  â€¢ Alert on exit code != 0

Log Management:
  â€¢ Create logs/ subdirectory (auto-created)
  â€¢ Keep last 30 days of logs (11MB/month)
  â€¢ Archive or delete old logs periodically
  â€¢ Store on network share for central monitoring


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… DO:
  â€¢ Check logs immediately after first run
  â€¢ Set up alerts on ERROR or WARNING
  â€¢ Review warnings before going to production
  â€¢ Archive logs for compliance/audit trail
  â€¢ Include log path in alerts/monitoring
  â€¢ Use log_context for new operations
  â€¢ Log data quality issues with counts

âŒ DON'T:
  â€¢ Ignore WARNING messages
  â€¢ Delete logs without archiving
  â€¢ Assume no errors if exit code is 0
  â€¢ Skip checking data quality warnings
  â€¢ Run without monitoring first time
  â€¢ Disable logging for "performance"
  â€¢ Log sensitive data (passwords, etc.)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ EXAMPLE PRODUCTION LOG ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Successful Run (what you want to see):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ETL PIPELINE COMPLETED SUCCESSFULLY

Extract Summary:
  users..................... 10000
  products.................. 2000
  orders.................... 20000
  order_items............... 43525
  events.................... 80000
  reviews................... 15000

Transform staging completed (0 duplicates dropped)
Warehouse build completed (170,525 total rows)
Load phase: All tables saved successfully
Validation: ALL CHECKS PASSED


Warning Example (data quality issue):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WARNING | Found 5 NULL user_ids (will cause issues)
WARNING | 10 invalid signup dates converted to NaT

â†“ Check and fix:
  - Review Data/Raw/users.csv for NULL user_ids
  - Correct date format or handle edge cases
  - Re-run pipeline


Error Example (failure):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ERROR | CSV parsing error in orders: [Error details]
ERROR | Failed to save dim_products: Permission denied

â†“ Diagnose:
  - Check file is readable/writable
  - Verify CSV format
  - Check disk space
  - Check user permissions


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ EXTENDING LOGGING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Add Logging to New Code:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Import at top of file:
   from .logger_config import get_logger
   logger = get_logger(__name__)

2. Use in functions:
   logger.info("Starting operation...")
   logger.warning("Data issue found")
   logger.error("Operation failed")

3. Or use context manager:
   with log_context(logger, "operation_name"):
       # Your code here
       pass


Custom Summary Table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from .logger_config import log_summary

log_summary(logger, "Row Counts", {
    "users": 10000,
    "products": 2000,
    "orders": 20000,
})

Output:
  Row Counts
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    users............................... 10000
    products............................ 2000
    orders............................. 20000
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ¨ SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your ETL pipeline is now PRODUCTION-READY with:

âœ… Comprehensive logging at every stage
âœ… Error handling with full tracebacks
âœ… Data quality warning detection
âœ… Context managers for operation tracking
âœ… Dual output (console + file)
âœ… Detailed audit trail for compliance
âœ… Easy-to-read structured format
âœ… Timestamp tracking per operation

When something breaks:
  1. Check console output for ERROR messages
  2. Open logs/ directory for detailed log file
  3. Search for ERROR, WARNING, or relevant stage name
  4. Full traceback shows exactly what failed
  5. Fix the issue and re-run

Ready for production deployment! ğŸš€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
